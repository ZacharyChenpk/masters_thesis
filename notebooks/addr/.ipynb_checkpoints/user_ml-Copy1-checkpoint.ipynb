{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# first_block = 400040\n",
    "# last_block = 400045\n",
    "\n",
    "with open('../pickles/ml/train_blocks', 'rb') as f:\n",
    "    block_list = pickle.load(f)\n",
    "\n",
    "class User:\n",
    "    def __init__(self, adrs, txs):\n",
    "        self.adr = set(adrs)\n",
    "        self.sending_tx = set(txs)\n",
    "        self.cadr = set()\n",
    "        self.receiving_tx = set(txs)\n",
    "        \n",
    "#Read Users found\n",
    "with open('../pickles/pool/all_users.pickle', 'rb') as f:\n",
    "    users = pickle.load(f)\n",
    "df = pd.read_pickle('../pickles/pool/all_df_with_users.pickle')\n",
    "\n",
    "# #Read Users found\n",
    "# with open('../pickles/pool/all_users.pickle', 'rb') as f:\n",
    "#     users = pickle.load(f)\n",
    "# df = pd.read_pickle('../pickles/pool/all_df_with_users.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user input features\n",
    "user_input_df = df.groupby('input_user').agg({\n",
    "    'id_txo_out': 'nunique', #Num unique times paid out\n",
    "    'oadr':'nunique', #Num of unique out addresses paid out\n",
    "    'output_user': 'nunique', #Num of unique users paid out (Out Degree)\n",
    "    #'id_txi': 'nunique', #Num unique times paid in\n",
    "    'id_t': 'nunique', #Num Txs involved in\n",
    "    'input_val': ['max', 'min']\n",
    "})\n",
    "\n",
    "user_input_df.columns = ['_'.join(col) for col in user_input_df.columns]\n",
    "\n",
    "user_input_df.rename(columns={\n",
    "    'id_txo_out_nunique': 'unique_sent',\n",
    "    'oadr_nunique': 'unique_sent_adr',\n",
    "    'output_user_nunique': 'unique_sent_user',  # (Out Degree)\n",
    "    'id_t_nunique': 'num_sending_tx',\n",
    "    'input_val_max': 'max_sent',\n",
    "    'input_val_min': 'min_sent'\n",
    "}, inplace=True)\n",
    "\n",
    "user_input_df['total_sent'] = (df['input_val'] / df['num_txo']).groupby(df['input_user']).sum()\n",
    "#dummy1 = (df['input_val'] / df['num_txo']).groupby(df['input_user']).sum()\n",
    "#dummy2 = (df['output_val'] / df['num_txi']).groupby(df['input_user']).sum()\n",
    "\n",
    "# user output features\n",
    "user_out_df = df.groupby('output_user').agg({\n",
    "    'id_txi': 'nunique', #Num unique times paid in\n",
    "    'iadr': 'nunique', #Num of unique in addresses paid this user\n",
    "    'input_user': 'nunique', #Num of unique users paid in (In Degree)\n",
    "    #'id_txo_out': 'nunique', #Num unique times paid\n",
    "    'id_t': 'nunique', #Num Txs involved in\n",
    "    'output_val': ['max', 'min']\n",
    "})\n",
    "\n",
    "user_out_df.columns = ['_'.join(col) for col in user_out_df.columns]\n",
    "\n",
    "user_out_df.rename(columns={\n",
    "    'id_txi_nunique': 'unique_rec',\n",
    "    'iadr_nunique': 'unique_rec_adr',\n",
    "    'input_user_nunique': 'unique_rec_user',  # (In Degree)\n",
    "    'id_t_nunique': 'num_receiving_tx',\n",
    "    'output_val_max': 'max_rec',\n",
    "    'output_val_min': 'min_rec'\n",
    "}, inplace=True)\n",
    "\n",
    "user_out_df['total_rec'] = (df['output_val'] / df['num_txi']).groupby(df['output_user']).sum()\n",
    "\n",
    "# Merge input and output user features\n",
    "user_df = user_input_df.merge(user_out_df, how='outer', left_index=True, right_index=True)\n",
    "user_df = user_df.iloc[:len(users)]\n",
    "\n",
    "# Name index\n",
    "user_df.index.name = 'user'\n",
    "\n",
    "# New columns\n",
    "user_df['num_tx'] = user_df['unique_sent_tx'] + user_df['unique_rec_tx']\n",
    "# user_df = user_df.drop(['tx1', 'tx2'], axis=1)\n",
    "\n",
    "temp = df.groupby('output_user').agg({\n",
    "    'iadr': lambda x: (x == '0').any(), #Num of unique in addresses paid this user\n",
    "})\n",
    "temp.rename(columns={\n",
    "    'iadr': 'is_miner',\n",
    "}, inplace=True)\n",
    "# Name index\n",
    "#temp.index.name = 'user'\n",
    "user_df['is_miner'] = temp['is_miner'].iloc[:len(users)]\n",
    "\n",
    "# Fill in NA values\n",
    "user_df['is_miner'].fillna(False, inplace=True)\n",
    "user_df.fillna(0, inplace=True)\n",
    "\n",
    "user_df.is_miner = user_df.is_miner.apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "\n",
    "#Add column to indicate labelled users\n",
    "user_df[\"category\"] = \"unknown\"\n",
    "\n",
    "user_total_sent = [(user,total_rec) for (user, total_rec) in user_df['total_sent'].iteritems()]\n",
    "#users_identified = list(user_df.index.values) \n",
    "\n",
    "# Totals\n",
    "total_amt_spent = (df['input_val'] / df['num_txo']).sum() ##Total amount spent in this block?\n",
    "total_amt_recieved = (df['output_val'] / df['num_txi']).sum() ##Total amount received in this block?\n",
    "\n",
    "user_df = user_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Users with data from walletexplorer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary structure - \n",
    "#'Address': 'Category'\n",
    "categories = ['gambling','pool']\n",
    "dic_userlabels = defaultdict(set)\n",
    "starttime = time.time() \n",
    "\n",
    "for block in range(first_block,last_block+1,1):\n",
    "#for category in categories:\n",
    "    for block in block_list:\n",
    "        category_df = pd.read_pickle('../pickles/categories/{}.pickle'.format(category))\n",
    "        addr = category_df.loc[category_df['last used in block'] == block]['address'].tolist()\n",
    "        if(len(addr)!=0):\n",
    "            for a in addr:\n",
    "                dic_userlabels[a].add(category)\n",
    "print(\"Total time to dict:\", time.time()-starttime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_label = defaultdict(set)\n",
    "# label = {}\n",
    "# label[categories[0]]=1\n",
    "# label[categories[1]]=2\n",
    "for i, user in enumerate(users):\n",
    "    labels = set()\n",
    "    for key in dic_userlabels:\n",
    "        if key in user.adr:\n",
    "            cat = dic_userlabels[key]\n",
    "            user_df.loc[i, 'category'] = list(cat)[0] #label[cat.pop()]\n",
    "            labels.add(list(cat)[0])\n",
    "    if labels:\n",
    "        user_label[i].update(labels)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_blocks = [len(_user.blocks) for _user in users]\n",
    "_num_adr = [len(_user.adr) for _user in users]\n",
    "user_df_normalized = user_df.copy()\n",
    "user_df_normalized['num_blocks_active'] = _blocks\n",
    "user_df_normalized['num_adr'] = _num_adr\n",
    "\n",
    "#user_df_normalized[['unique_sent_user','unique_sent', 'unique_sent_adr', 'unique_sent_tx', 'total_sent', 'unique_rec', 'unique_rec_tx', 'unique_rec_user', 'unique_rec_adr', 'total_rec', 'num_tx']] = user_df[['unique_sent_user','unique_sent', 'unique_sent_adr', 'unique_sent_tx', 'total_sent', 'unique_rec', 'unique_rec_tx', 'unique_rec_user', 'unique_rec_adr', 'total_rec', 'num_tx']].div(_blocks, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlation = user_df.corr(method='spearman')\n",
    "print(\"correlation: \")\n",
    "data_correlation.style.format(\"{:.2}\").background_gradient(cmap=plt.cm.Greens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlation[(abs(data_correlation) > 0.8) & ( data_correlation != 1.0)].dropna(how=\"all\", axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = data_correlation.index\n",
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(data_correlation, vmin=-1, vmax=1, cmap=plt.cm.Greens)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(indices),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xticklabels(indices, fontsize=8)\n",
    "ax.set_yticklabels(indices, fontsize=8)\n",
    "plt.grid()\n",
    "plt.savefig(\"correlation_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imbalanced dataset! - factor of ~10\n",
    "gambling = np.sum(user_df[\"category\"] == 'gambling') # or just sum over column, bc its binary\n",
    "pool = np.sum(user_df[\"category\"] == 'pool')\n",
    "unknown = np.sum(user_df[\"category\"] == 'unknown')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gambling, pool,unknown = plt.bar((0,1,2), (gambling, pool,unknown))\n",
    "gambling.set_facecolor('dimgray')\n",
    "pool.set_facecolor('lightgray')\n",
    "ax.set_xticks((0,1,2))\n",
    "for p in (ax.patches):\n",
    "    ax.annotate(str(p.get_height()), (p.get_x()+0.34, p.get_height() +100), fontsize=10)\n",
    "ax.set_xticklabels(['Gambling service', 'Mining pool', 'Unkown User', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_by_category = user_df.groupby(by=\"category\").median()\n",
    "mean_by_category = user_df.groupby(by=\"category\").mean()\n",
    "most_rel_diff_median = (median_by_category.diff().iloc[1])/(median_by_category.max()+1e-7) # get the once that differ relatively the most\n",
    "most_rel_diff_median = most_rel_diff_median[abs(most_rel_diff_median) > 0.04].sort_values(ascending=False)\n",
    "rel_max = median_by_category[most_rel_diff_median.index.values]/median_by_category[most_rel_diff_median.index.values].max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = rel_max.shape[1]\n",
    "GamblingMedian = rel_max.iloc[0].values\n",
    "PoolMedian = rel_max.iloc[1].values\n",
    "UnknownMedian = rel_max.iloc[2].values\n",
    "ind = np.arange(N)    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "index = np.arange(N)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 1\n",
    "\n",
    "ax.bar(index, GamblingMedian, bar_width, alpha=opacity,\n",
    "                color='#1F936D', label='Users that are gambling services')\n",
    "\n",
    "\n",
    "ax.bar(index + bar_width, PoolMedian, bar_width, alpha=opacity,\n",
    "                color='#36D588', label='Users that are pooling services')\n",
    "\n",
    "\n",
    "ax.plot((most_rel_diff_median.values), color=\"r\", linestyle=\"--\", label = \"relative difference\")\n",
    "\n",
    "# annotate line chart\n",
    "for i,j in enumerate((most_rel_diff_median.values)):\n",
    "    ax.annotate(str(np.round(j,2)),xy=(i,j+0.01), fontsize=12)\n",
    "    \n",
    "# annotate bar chart\n",
    "len_median_by_category = len(median_by_category[most_rel_diff_median.index.values].iloc[0])\n",
    "for i,p in enumerate(ax.patches):\n",
    "    if i < len_median_by_category:\n",
    "        c_bar_value = int(median_by_category[most_rel_diff_median.index.values].iloc[0][i])\n",
    "    else: \n",
    "        c_bar_value = int(median_by_category[most_rel_diff_median.index.values].iloc[1][i-len_median_by_category])\n",
    "    ax.annotate(str(c_bar_value), (p.get_x(), p.get_height() * 1.005), fontsize=12)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Relative difference')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.set_xticklabels(rel_max.columns)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test set via downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unknown = user_df[user_df[\"category\"] == 'unknown']#.drop(columns=[\"category\"])\n",
    "train_gambling = user_df[user_df[\"category\"] == 'gambling']#.drop(columns=[\"category\"])\n",
    "train_pool = user_df[user_df[\"category\"] == 'pool']#.drop(columns=[\"category\"])\n",
    "\n",
    "# balance the dataset: \n",
    "indices1 = np.arange(train_unknown.shape[0])\n",
    "#indices2 = np.arange(train_gambling.shape[0])\n",
    "random_indices1 = np.random.choice(indices1, size = 300)\n",
    "#random_indices2 = np.random.choice(indices2, size = train_pool.shape[0])\n",
    "train_unknown = train_unknown.iloc[random_indices1] # choose random staying customers\n",
    "X_train_downsampled = pd.concat((pd.concat((train_gambling,train_unknown)),train_pool))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "X_train_downsampled = X_train_downsampled[~X_train_downsampled.index.duplicated(keep='first')]\n",
    "cats = X_train_downsampled.category\n",
    "X_train_downsampled = X_train_downsampled.drop(columns=[\"category\"])\n",
    "y_train_downsampled = LabelEncoder().fit_transform(cats)\n",
    "\n",
    "X_train_downsampled, X_test_downsampled, y_train_downsampled, y_test_downsampled = train_test_split(X_train_downsampled, y_train_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unknown = user_df[user_df[\"category\"] == 'unknown'].drop(columns=[\"category\"])\n",
    "y_result = dec_tree_classifier.predict(x_unknown)\n",
    "y_actual = LabelEncoder().fit_transform(user_df[user_df[\"category\"] == 'unknown'].category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_result, y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced churn set: \")\n",
    "# imbalanced dataset! - factor of ~10\n",
    "gambling = np.sum(cats == 'gambling') # or just sum over column, bc its binary\n",
    "pool = np.sum(cats == 'pool')\n",
    "unknown = np.sum(cats == 'unknown')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gambling, pool,unknown = plt.bar((0,1,2), (gambling, pool, unknown))\n",
    "gambling.set_facecolor('dimgray')\n",
    "pool.set_facecolor('lightgray')\n",
    "ax.set_xticks((0,1,2))\n",
    "\n",
    "ax.set_xticklabels(['Gambling', 'Pool', 'Unkown'])\n",
    "ax.set_ylabel('Number of users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, filename): \n",
    "    plt.figure()\n",
    "    mat = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(mat, square=True, annot=True, fmt='d', cmap=\"YlGnBu\", cbar=True)\n",
    "    plt.ylabel('true label'),\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.savefig(filename+\".pdf\")\n",
    "    plt.figure()\n",
    "    mat_normalized = mat / mat.sum(axis=1)\n",
    "    sns.heatmap(mat_normalized, fmt=\"f\", square=True, annot=True, cmap=\"YlGnBu\", cbar=True)\n",
    "    plt.ylabel('true label'),\n",
    "    plt.xlabel('predicted label')\n",
    "    print(\"confusion matrix normalized: \")\n",
    "    plt.savefig(filename+\"_normalized.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train and test set via upsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unknown = user_df[user_df[\"category\"] == 0].drop(columns=[\"category\"])\n",
    "train_gambling = user_df[user_df[\"category\"] == 1].drop(columns=[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unbalanced = np.concatenate((train_unknown, train_gambling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unbalanced = np.concatenate((np.zeros(train_unknown.shape[0]),np.ones(train_gambling.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled, X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_unbalanced, y_unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_not_service_in_upsampled_train = np.sum(y_train_upsampled == 0)\n",
    "n_service_in_upsampled_train = np.sum(y_train_upsampled == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled, y_train_upsampled = SMOTE(random_state=42, ratio={0:n_not_service_in_upsampled_train, 1:n_not_service_in_upsampled_train}).fit_sample(X_train_upsampled, y_train_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_not_service_in_upsampled_test = np.sum(y_test_upsampled == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_upsampled, y_test_upsampled = SMOTE(random_state=42, ratio={0:n_not_service_in_upsampled_test, 1:n_not_service_in_upsampled_test}).fit_sample(X_test_upsampled, y_test_upsampled)n_staying_in_upsampled_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree on downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pydot\n",
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_dec_tree(X_train,y_train,X_test, y_test):\n",
    "    best_acc = 0\n",
    "    best_depth = 0\n",
    "    for i in range(3,20):\n",
    "        dec_tree = tree.DecisionTreeClassifier(max_depth = i, random_state=42)\n",
    "        dec_tree = dec_tree.fit(X_train, y_train)\n",
    "        y_pred = dec_tree.predict(X_test)\n",
    "        acc_score = accuracy_score(y_test, y_pred)\n",
    "        if acc_score > best_acc: \n",
    "            best_acc = acc_score\n",
    "            best_depth = i\n",
    "    dec_tree = tree.DecisionTreeClassifier(max_depth = best_depth, random_state=42)\n",
    "    dec_tree.fit(X_train, y_train)\n",
    "    y_pred = dec_tree.predict(X_test)\n",
    "    print(\"the accuracy = \" + str(accuracy_score(y_test, y_pred)))\n",
    "    return dec_tree, y_pred\n",
    "\n",
    "def plot_dec_tree(decision_tree, feature_names, filename):\n",
    "    dot_data = tree.export_graphviz(decision_tree, out_file=None, \n",
    "                                feature_names=feature_names,\n",
    "                                filled=True,\n",
    "                                rounded=True) \n",
    "    graph = pydot.graph_from_dot_data(dot_data)\n",
    "    graph[0].write_png(filename+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_classifier, y_pred = get_best_dec_tree(X_train=X_train_downsampled, y_train=y_train_downsampled,\n",
    "                                       X_test=X_test_downsampled, y_test=y_test_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving confusion matrix for decision tree...\")\n",
    "plot_confusion_matrix(y_test_downsampled, y_pred, \"decision_tree_downsampled_conf_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_classifier.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dec_tree(dec_tree_classifier, feature_names=X_train_downsampled.columns, filename=\"decision_tree_downsampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \", accuracy_score(y_test_downsampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test_downsampled, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test_downsampled, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree on upsampled data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_classifier, y_pred = get_best_dec_tree(X_train=X_train_upsampled, y_train=y_train_upsampled,\n",
    "                                       X_test=X_test_upsampled, y_test=y_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving confusion matrix for decision tree...\")\n",
    "plot_confusion_matrix(y_test_upsampled, y_pred, \"decision_tree_upsampled_conf_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \", accuracy_score(y_test_upsampled, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test_upsampled, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test_upsampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"save decision tree with few depth layers for visualization: \")\n",
    "dec_tree = tree.DecisionTreeClassifier(max_depth = 4, random_state=42)\n",
    "dec_tree = dec_tree.fit(X_train_upsampled, y_train_upsampled)\n",
    "y_pred = dec_tree.predict(X_test_upsampled)\n",
    "print(accuracy_score(y_test_upsampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dec_tree(dec_tree_classifier, feature_names=X_train_downsampled.columns, filename=\"decision_tree_upsampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "Y = pd.DataFrame(user_df['category'])\n",
    "user_df_x = user_df.drop(['category'], axis=1)\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(user_df_x), columns=user_df_x.columns, index=user_df_x.index)\n",
    "\n",
    "#Correlation\n",
    "data_corr = user_df_x.corr()\n",
    "sns.heatmap(data_corr, cmap = 'bwr') #heatmap of correlation matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
