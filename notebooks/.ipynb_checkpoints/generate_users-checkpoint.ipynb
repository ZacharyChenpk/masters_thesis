{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from functools import reduce\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# first_block = 389646\n",
    "# last_block = 401221\n",
    "\n",
    "service_df = pd.read_pickle('../pickles/services/pools.pickle')\n",
    "x = service_df['last used in block'].value_counts()\n",
    "x = x.to_frame()\n",
    "x = x.iloc[:50]\n",
    "block_list = set(x.index)\n",
    "block_list = sorted(block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, adrs, txs):\n",
    "        self.adr = set(adrs)\n",
    "        self.sending_tx = set(txs)\n",
    "        #self.cadr = set()\n",
    "        self.receiving_tx = set(txs)\n",
    "        \n",
    "#CHANGE LATER TO QUERY DATABASE INSTEAD\n",
    "def iadrs_from_tx(id_t):\n",
    "    return set(df['iadr'][df[\"id_t\"] == id_t])\n",
    "\n",
    "def oadrs_from_tx(id_t):\n",
    "    return set(df['oadr'][df[\"id_t\"] == id_t])\n",
    "\n",
    "def tx_from_iadr(iadr):\n",
    "    return set(df['id_t'][df[\"iadr\"] == iadr])\n",
    "\n",
    "#FUNCTION TO WHICH YOU GIVE AN INPUT ADDRESS AND GET USER \n",
    "def get_user(input_adr):\n",
    "    to_inv = [input_adr]\n",
    "    user_iadrs = set()\n",
    "    seen_txs = set()\n",
    "    while to_inv:\n",
    "        current_iadr = to_inv.pop(0)\n",
    "        user_iadrs.add(current_iadr)\n",
    "        for id_t in tx_from_iadr(current_iadr):\n",
    "            if id_t not in seen_txs:\n",
    "                seen_txs.add(id_t)\n",
    "                iadrs = iadrs_from_tx(id_t)\n",
    "                to_inv += iadrs.difference(user_iadrs) #Adding addr\n",
    "                user_iadrs.update(iadrs)\n",
    "            \n",
    "    return User(user_iadrs, seen_txs)\n",
    "\n",
    "#TAKES OUTPUT ADDRESS AND GIVES USER THAT HAS THAT ADDRESS AS INPUT\n",
    "def user_from_oadr(oadr):\n",
    "    for i,user in enumerate(users):\n",
    "        if oadr in user.adr:\n",
    "            return i        \n",
    "\n",
    "def get_cadr(user, potential_cadrs):\n",
    "    cadrs_for_user = set()\n",
    "    for tx_id in user.sending_tx:    #CHECK THIS\n",
    "        o = oadrs_from_tx(tx_id)\n",
    "        for oadr in o:\n",
    "            if oadr in potential_cadrs and oadr not in user.adr:\n",
    "                user.adr.add(oadr)\n",
    "                potential_cadrs.remove(oadr) #Ensure that same change address won't be another user\n",
    "    return user, potential_cadrs\n",
    "\n",
    "#df = pd.read_pickle('../pickles/df/{}_to_{}.pickle'.format(first_block,last_block))\n",
    "df = pd.read_pickle('../pickles/df/all_blocks.pickle')\n",
    "#df = pd.read_pickle('../pickles/df/400000.pickle')\n",
    "#df = pd.read_csv('./csv/400000addr.csv')\n",
    "#df[[col for col in df.columns if not 'Unnamed' in col]]\n",
    "\n",
    "# New columns for number of input and output transaction ids\n",
    "df['num_txo'] = df.groupby('id_t')['id_txo_out'].transform('nunique')\n",
    "df['num_txi'] = df.groupby('id_t')['id_txi'].transform('nunique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otc_dic = {}\n",
    "for block in block_list:\n",
    "#for block in range(first_block,last_block+1,1):\n",
    "    if(os.path.exists(\"../pickles/df/{}.pickle\".format(block)) and os.path.exists(\"../pickles/otc/otc_{}.pickle\".format(block))):\n",
    "        with open ('../pickles/otc/otc_{}.pickle'.format(block), 'rb') as fp:\n",
    "            otc_dic[block] = pickle.load(fp)\n",
    "    \n",
    "not_seen = list(reduce(set.symmetric_difference, (set(val) for val in otc_dic.values())))\n",
    "\n",
    "potential_cadrs = set()\n",
    "\n",
    "for tx_id in set(df.id_t):    #CHECK THIS\n",
    "    o = oadrs_from_tx(tx_id)\n",
    "    potential_cadr = []\n",
    "    for oadr in o:\n",
    "        if oadr in not_seen:\n",
    "            potential_cadr.append(oadr)\n",
    "    if len(potential_cadr)==1:\n",
    "        potential_cadrs.add(potential_cadr[0])\n",
    "        not_seen.remove(potential_cadr[0]) #Ensure that same change address won't be assigned to another user\n",
    "\n",
    "potential_cadrs = list(potential_cadrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIST OF LISTS OF USER AND THEIR ASSOCIATED ADDRESSES\n",
    "users = []\n",
    "\n",
    "starttime = time.time()        \n",
    "        \n",
    "# Bitcoin- \n",
    "bitcoin = User({str(0)}, set()) #Make user object with bitcoin iadr (which is 0)\n",
    "seen_miner_iadrs_tx = defaultdict(set) #Make dict associating miner payment address with tx_ids they've been involved in\n",
    "\n",
    "for index, row in df[df['iadr'] == str(0)].iterrows(): #Going through all mining txs\n",
    "    bitcoin.sending_tx.add(row['id_t']) #Adding tx id to bitcoin user's txs\n",
    "    seen_miner_iadrs_tx[row['oadr']].add(row['id_t']) # Updating dict to register the tx_id as corresponding to miners adr. If new, then new key added, otherwise added to values of existing key\n",
    "\n",
    "users.append(bitcoin) # Add bitcoin user\n",
    "already_seen_iadr = {'0'}  # Bitcoin iadr has already been seen\n",
    "\n",
    "# make miners users\n",
    "for adr, id_ts in seen_miner_iadrs_tx.items(): #Go through dictionary for every adr (miner) and txs he's been involved\n",
    "    miner = get_user(adr) #From an address, give back user ... aka identify all addresses belonging to miner\n",
    "    miner.adr.add(adr) #Make sure adrs and txs are added in\n",
    "    miner.receiving_tx.update(id_ts)\n",
    "    miner, potential_cadrs = get_cadr(miner, potential_cadrs)\n",
    "    users.append(miner)\n",
    "    already_seen_iadr.update(miner.adr)#Made sure miner's addresses are in already seen so that we don't create a second user with the same addresses\n",
    "\n",
    "## ASSOCIATE INPUT ADDRESS AND TX WITH EACH USER IN BLOCK\n",
    "# make other users from heuristic\n",
    "for input_adr in df.iadr:\n",
    "    if input_adr not in already_seen_iadr:\n",
    "        user = get_user(input_adr)\n",
    "        user, potential_cadrs = get_cadr(user, potential_cadrs)\n",
    "        users.append(user)\n",
    "        already_seen_iadr.update(user.adr)\n",
    "\n",
    "edges = defaultdict(int)\n",
    "\n",
    "#WHICH USERS IN HAVE TRANSACTED WITH EACH OTHER\n",
    "for i,user in enumerate(users):\n",
    "    for tx_id in user.sending_tx:\n",
    "        for oadr in oadrs_from_tx(tx_id):\n",
    "            if oadr in already_seen_iadr:\n",
    "\n",
    "                    edges[(i, user_from_oadr(oadr))]+=1  \n",
    "                    \n",
    "print(\"Total time to process heuristic 1:\", time.time()-starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCouples(users):\n",
    "    to_combine = []\n",
    "    for i, user in enumerate(users):\n",
    "        for k, otheruser in enumerate(users):\n",
    "            if user.adr.intersection(otheruser.adr) and i!=k:\n",
    "                to_combine.append((i,k))\n",
    "    return to_combine\n",
    "\n",
    "print(len(users))\n",
    "to_combine = getCouples(users) \n",
    "while to_combine:\n",
    "    print(\"to_combine length \", len(to_combine))\n",
    "    st = set(users)\n",
    "    for tple in to_combine:\n",
    "        user1 = users[tple[0]]\n",
    "        user2 = users[tple[1]]\n",
    "        if user1 in st and user2 in st:\n",
    "            #print(tple)\n",
    "            st.remove(user1)\n",
    "            st.remove(user2)\n",
    "            user1.adr = user1.adr.union(user2.adr)\n",
    "            st.add(user1)\n",
    "    users = list(st)\n",
    "    print(len(users))\n",
    "    to_combine = getCouples(users)\n",
    "\n",
    "#Save Users found\n",
    "#with open('../pickles/users/users_{}_to_{}.pickle'.format(first_block,last_block),'wb') as f:\n",
    "with open('../pickles/users/all_users.pickle','wb') as f:    \n",
    "    pickle.dump(users,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tx_ids = []\n",
    "for user in users[1:]:#Drop out bitcoin user\n",
    "    tx_ids += list(user.sending_tx)\n",
    "\n",
    "#tx_ids = list.append([list(user.sending_tx) for user in users])\n",
    "        \n",
    "ads = []\n",
    "for i,user in enumerate(users):\n",
    "    for ad in user.adr:\n",
    "        ads.append(ad)\n",
    "        \n",
    "# cads = []\n",
    "# for i,user in enumerate(users):\n",
    "#     for cad in user.cadr:\n",
    "#         cads.append(cad)\n",
    "\n",
    "if Counter(tx_ids).most_common(10)[0][1] == 1:  ##Repeated txids between bitcoin and the miners\n",
    "    print(\"No repeats txid\")\n",
    "    \n",
    "if Counter(ads).most_common(10)[0][1] == 1:\n",
    "    print(\"No repeats ads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read Users found\n",
    "# with open('../pickles/users/users_{}_to_{}.pickle'.format(first_block,last_block), 'rb') as f:\n",
    "#     users = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('id_t').agg({'oadr':['nunique', 'count'],'iadr':['nunique', 'count'],'id_txo_in':['nunique', 'count'],'id_txo_out':['nunique', 'count']})\n",
    "#df.to_csv('users.csv', columns=['input_user','output_user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions between users (constructing user graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct User Graph\n",
    "df['input_user'] = df['iadr']  \n",
    "df['output_user'] = df['oadr']\n",
    "\n",
    "starttime = time.time() \n",
    "#Replacing all input addresses and output addresses with a user corresponding to that address\n",
    "for i, user in enumerate(users): \n",
    "    #assert(isinstance(i, int))\n",
    "    df['input_user'] = df['input_user'].apply(lambda x: i if x in user.adr else x)\n",
    "    df['output_user'] = df['output_user'].apply(lambda x: i if x in user.adr else x)\n",
    "    \n",
    "for tx_id, output_user in df[['id_t','output_user']].values:\n",
    "    if isinstance(output_user,int):\n",
    "        users[output_user].receiving_tx.add(tx_id)\n",
    "print(\"Total time to construct user graph:\", time.time()-starttime)\n",
    "\n",
    "#df.to_pickle(\"../pickles/df/{}_to_{}_users.pickle\".format(first_block,last_block))\n",
    "df.to_pickle(\"../pickles/df/all_df_with_users.pickle\")\n",
    "#Save Users found\n",
    "#with open('../pickles/users/users_{}_to_{}.pickle'.format(first_block,last_block),'wb') as f:\n",
    "with open('../pickles/users/all_users.pickle','wb') as f: \n",
    "    pickle.dump(users,f)\n",
    "    \n",
    "# can't trust input_val column now\n",
    "# because dropped lots of inputs\n",
    "edges_df0 = df.drop_duplicates(['input_user', 'id_txo_out'])\n",
    "edges_df = edges_df0.groupby(['input_user', 'output_user']).apply(lambda group: group['output_val'].sum()).reset_index()\n",
    "#edges_df0 = edges_df0.rename(columns={0: 'edge_amount'})\n",
    "\n",
    "# edges_df2 = (\n",
    "#     df.groupby(['input_user', 'output_user'])\n",
    "#     .apply(lambda group: (group['output_val'] / group['num_txi']).sum())\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# assert((edges_df == edges_df2).all())\n",
    "\n",
    "# tups = []\n",
    "# for i in range(0, edges_df.shape[0]):\n",
    "#     tups.append((edges_df.at[i, 'input_user'],edges_df.at[i, 'output_user']))\n",
    "    \n",
    "tups = [(input_user, output_user, amount) for (index, input_user, output_user, amount) in edges_df.itertuples()]\n",
    "\n",
    "#with open(\"../pickles/user_graphs/{}_to_{}_users.pickle\".format(first_block,last_block), 'wb') as f:\n",
    "with open('../pickles/user_graphs/all_users_graph.pickle','wb') as f: \n",
    "    pickle.dump(tups,f)\n",
    "#tups2 = [(input_user, output_user, amount) for (index, input_user, output_user, amount) in edges_df2.itertuples()]\n",
    "\n",
    "# assert(all(tups == tups2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "# df['iadr'].apply(type).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
