{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the database using iGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.decomposition\n",
    "import sklearn.model_selection\n",
    "import sklearn.cluster\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import py2neo\n",
    "import seaborn as sb ##includes convenient heatmaps and boxplots\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "import igraph\n",
    "import cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query):\n",
    "    # REMEMBER TO BE CONNECTED TO IMPERIAL WIFI!\n",
    "    graph_db = py2neo.Graph(\"https://dsi-bitcoin.doc.ic.ac.uk:7473/db/data/\", auth=(\"guest_ro\", \"imperialO_nly\"))\n",
    "    return graph_db.run(query)\n",
    "\n",
    "def get_block_data(first_block, last_block):\n",
    "    query_string = \"\"\"\n",
    "                    MATCH (b:Block) <-[:MINED_IN]- (t:Tx) <-[:IN]- (txi:TxIn) <-[:UNLOCK]- (iadr:Address)\n",
    "                    WHERE b.height >= {} AND b.height <= {}\n",
    "                    MATCH (txi) <-[:SPENT]- (txo_in:TxOut) \n",
    "                    MATCH (oadr:Address) <-[:LOCK]- (txo_out:TxOut) <-[:OUT]- (t) \n",
    "                    \n",
    "                    RETURN iadr.address as iadr, oadr.address as oadr, txo_in.value as input_val, txo_out.value as output_val, ID(txo_in) as id_txo_in, ID(txi) as id_txi, ID(t) as id_t, ID(txo_out) as id_txo_out\n",
    "                    \n",
    "                    \"\"\".format(first_block, last_block)\n",
    "    return query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = query_database(get_block_data(400000,400000))\n",
    "#df = result.to_data_frame()\n",
    "df = pd.read_csv('block_400000.csv')\n",
    "#df[[col for col in df.columns if not 'Unnamed' in col]].head()\n",
    "\n",
    "# tups1 = []\n",
    "# tups2 = []\n",
    "# for d in result:\n",
    "#     tups1.append((d['iadr'],d['oadr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding users by using heuristic measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#CHANGE LATER TO QUERY DATABASE INSTEAD\n",
    "def iadrs_from_tx(id_t):\n",
    "    return set(df['iadr'][df[\"id_t\"] == id_t])\n",
    "\n",
    "def oadrs_from_tx(id_t):\n",
    "    return set(df['oadr'][df[\"id_t\"] == id_t])\n",
    "\n",
    "def tx_from_iadr(iadr):\n",
    "    return set(df['id_t'][df[\"iadr\"] == iadr])\n",
    "\n",
    "#FUNCTION TO WHICH YOU GIVE AN INPUT ADDRESS AND GET USER \n",
    "def get_user(input_adr):\n",
    "    to_inv = [input_adr]\n",
    "    user_iadrs = set()\n",
    "    seen_txs = set()\n",
    "    while to_inv:\n",
    "        current_iadr = to_inv.pop(0)\n",
    "        user_iadrs.add(current_iadr)\n",
    "        for id_t in tx_from_iadr(current_iadr):\n",
    "\n",
    "            if id_t not in seen_txs:\n",
    "                \n",
    "                seen_txs.add(id_t)\n",
    "                iadrs = iadrs_from_tx(id_t)\n",
    "                to_inv += iadrs.difference(user_iadrs) #Adding addr\n",
    "                user_iadrs.update(iadrs)\n",
    "            \n",
    "    return User(user_iadrs, seen_txs)\n",
    "\n",
    "#LIST OF LISTS OF USER AND THEIR ASSOCIATED ADDRESSES\n",
    "users = []\n",
    "\n",
    "class User:\n",
    "    def __init__(self, adrs, txs):\n",
    "        self.adr = set(adrs)\n",
    "        self.tx = set(txs)\n",
    "        self.cadr = set()\n",
    "\n",
    "# Bitcoin- \n",
    "bitcoin = User({str(0)}, set()) #Make user object with bitcoin iadr (which is 0)\n",
    "seen_miner_iadrs_tx = defaultdict(set) #Make dict associating miner payment address with tx_ids they've been involved in\n",
    "\n",
    "for index, row in df[df['iadr'] == str(0)].iterrows(): #Going through all mining txs \n",
    "    bitcoin.tx.add(row['id_t']) #Adding tx id to bitcoin user's txs\n",
    "    seen_miner_iadrs_tx[row['oadr']].add(row['id_t']) # Updating dict to register the tx_id as corresponding to miners adr. If new, then new key added, otherwise added to values of existing key\n",
    "    \n",
    "users.append(bitcoin) # Add bitcoin user\n",
    "already_seen_iadr = {str(0)}  # Bitcoin iadr has already been seen\n",
    "\n",
    "# make miners users\n",
    "for adr, id_ts in seen_miner_iadrs_tx.items(): #Go through dictionary for every adr (miner) and txs he's been involved\n",
    "    miner = get_user(adr) #From an address, give back user ... aka identify all addresses belonging to miner\n",
    "    miner.adr.add(adr) #Make sure adrs and txs are added in\n",
    "    miner.tx.update(id_ts)\n",
    "    users.append(miner)\n",
    "    already_seen_iadr.update(miner.adr)#Made sure miner's addresses are in already seen so that we don't create a second user with the same addresses\n",
    "    \n",
    "\n",
    "## ASSOCIATE INPUT ADDRESS AND TX WITH EACH USER IN BLOCK\n",
    "# make other users from heuristic\n",
    "for input_adr in df.iadr:\n",
    "    if input_adr not in already_seen_iadr:\n",
    "        user = get_user(input_adr)\n",
    "        users.append(user)\n",
    "        already_seen_iadr.update(user.adr)\n",
    "\n",
    "\n",
    "#TAKES OUTPUT ADDRESS AND GIVES USER THAT HAS THAT ADDRESS AS INPUT\n",
    "def user_from_oadr(oadr):\n",
    "    for i,user in enumerate(users):\n",
    "        if oadr in user.adr:\n",
    "            return i\n",
    "        \n",
    "edges = defaultdict(int)\n",
    "\n",
    "#WHICH USERS IN HAVE TRANSACTED WITH EACH OTHER\n",
    "for i,user in enumerate(users):\n",
    "    for tx_id in user.tx:\n",
    "        for oadr in oadrs_from_tx(tx_id):\n",
    "            if oadr in already_seen_iadr:\n",
    "\n",
    "                    edges[(i, user_from_oadr(oadr))]+=1  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('outfile', 'rb') as fp:\n",
    "    not_seen = pickle.load(fp)\n",
    "    \n",
    "appeared_once_o= list(df.oadr.value_counts()[df.oadr.value_counts()==1].index)\n",
    "all_iadrs= list(df.iadr.value_counts().index)\n",
    "\n",
    "o_never_used_as_i = set(appeared_once_o).difference(all_iadrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tx_ids = []\n",
    "for user in users[1:]:#Drop out bitcoin user\n",
    "    tx_ids += list(user.tx)\n",
    "\n",
    "#tx_ids = list.append([list(user.tx) for user in users])\n",
    "        \n",
    "ads = []\n",
    "for i,user in enumerate(users):\n",
    "    for ad in user.adr:\n",
    "        ads.append(ad)\n",
    "        \n",
    "cads = []\n",
    "for i,user in enumerate(users):\n",
    "    for cad in user.cadr:\n",
    "        cads.append(cad)\n",
    "\n",
    "if Counter(tx_ids).most_common(10)[0][1] == 1:  ##Repeated txids between bitcoin and the miners\n",
    "    print(\"No repeats txid\")\n",
    "    \n",
    "if Counter(ads).most_common(10)[0][1] == 1:\n",
    "    print(\"No repeats ads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,user in enumerate(users):\n",
    "    temp = set()\n",
    "    for tx_id in user.tx:\n",
    "        o = oadrs_from_tx(tx_id)\n",
    "        potential_cadr = []\n",
    "        for oadr in o:\n",
    "            if oadr in not_seen and oadr in o_never_used_as_i:\n",
    "                potential_cadr.append(oadr)\n",
    "        if len(potential_cadr)==1:\n",
    "            temp.add(potential_cadr[0])\n",
    "    user.cadr.update(temp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('id_t').agg({'oadr':['nunique', 'count'],'iadr':['nunique', 'count'],'id_txo_in':['nunique', 'count'],'id_txo_out':['nunique', 'count']})\n",
    "\n",
    "df.to_csv('users.csv', columns=['input_user','output_user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct User Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct User Graph\n",
    "df['input_user'] = df['iadr']\n",
    "df['output_user'] = df['oadr']\n",
    "for i, user in enumerate(users):\n",
    "    assert(isinstance(i, int))\n",
    "    df['input_user'] = df['input_user'].apply(lambda x: i if x in user.adr else x)\n",
    "    df['output_user'] = df['output_user'].apply(lambda x: i if x in user.adr else x)\n",
    "    df['output_user'] = df['output_user'].apply(lambda x: i if x in user.cadr else x)\n",
    "\n",
    "# can't trust input_val column now\n",
    "# because dropped lots of inputs\n",
    "df_new = df.drop_duplicates(['input_user', 'id_txo_out'])\n",
    "df_new = df_new.groupby(['input_user', 'output_user']).apply(lambda group: group['output_val'].sum()).reset_index()\n",
    "tups = []\n",
    "for i in range(0, df_new.shape[0]):\n",
    "    tups.append((df_new.at[i, 'input_user'],df_new.at[i, 'output_user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df['iadr'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_txo'] = df.groupby('id_t')['id_txo_out'].transform('nunique')\n",
    "df['num_txi'] = df.groupby('id_t')['id_txi'].transform('nunique')\n",
    "\n",
    "user_df = df.groupby('input_user').agg({\n",
    "    'id_txo_out': 'nunique', #Num unique times paid out\n",
    "    'oadr':'nunique', #Num of unique out addresses paid out\n",
    "    'output_user': 'nunique', #Num of unique users paid out (Out Degree)\n",
    "    #'id_txi': 'nunique', #Num unique times paid in\n",
    "    'id_t': 'nunique', #Num Txs involved in\n",
    "    'input_val': ['max', 'min']\n",
    "})\n",
    "\n",
    "user_df['input_val','sum'] = (df['input_val'] / df['num_txo']).groupby(df['input_user']).sum()\n",
    "#dummy1 = (df['input_val'] / df['num_txo']).groupby(df['input_user']).sum()\n",
    "#dummy2 = (df['output_val'] / df['num_txi']).groupby(df['input_user']).sum()\n",
    "\n",
    "#print(user_df.tail())\n",
    "\n",
    "user_out_df = df.groupby('output_user').agg({\n",
    "    'id_txi': 'nunique', #Num unique times paid in\n",
    "    'iadr':'nunique', #Num of unique in addresses paid this user\n",
    "    'input_user': 'nunique', #Num of unique users paid in (In Degree)\n",
    "    #'id_txo_out': 'nunique', #Num unique times paid\n",
    "    'id_t': 'nunique', #Num Txs involved in\n",
    "    'output_val': ['max', 'min']\n",
    "})\n",
    "\n",
    "user_out_df['output_val','sum'] = (df['output_val'] / df['num_txi']).groupby(df['output_user']).sum()\n",
    "\n",
    "#df['output_user'].apply(type).value_counts()\n",
    "\n",
    "user_df = user_df.merge(user_out_df, how='left', left_index=True, right_index=True)\n",
    "user_df = user_df.append(user_out_df.loc[1])\n",
    "user_df.fillna(0,inplace=True)\n",
    "\n",
    "total_amt_spent = (df['input_val'] / df['num_txo']).sum() ##Total amount spent in this block?\n",
    "total_amt_recieved = (df['output_val'] / df['num_txi']).sum() ##Total amount received in this block?\n",
    "\n",
    "user_df.columns = [' '.join(col).strip() for col in user_df.columns.values]\n",
    "user_df.columns.tolist()\n",
    "\n",
    "user_df.rename(columns={'id_t_x nunique': 'tx1', \n",
    "                   'id_txo_out nunique': 'unique_sent',\n",
    "                   'output_user nunique': 'unique_sent_user (Out Degree)',\n",
    "                   'input_val max': 'max_sent',\n",
    "                   'input_val min': 'min_sent',\n",
    "                   'oadr nunique': 'unique_sent_adr',\n",
    "                   'input_val sum': 'total_sent',\n",
    "                   'iadr nunique': 'unique_rec_adr',\n",
    "                   'output_val max': 'max_rec',\n",
    "                   'output_val min': 'min_rec',\n",
    "                   'id_t_y nunique': 'tx2',\n",
    "                   'input_user nunique': 'unique_rec_user (In Degree)',\n",
    "                   'id_txi nunique': 'unique_rec',\n",
    "                   'output_val sum': 'total_rec',\n",
    "                  }, inplace=True)\n",
    "\n",
    "user_df['num_tx'] = user_df['tx1'] + user_df['tx2']\n",
    "user_df = user_df.drop(['tx1', 'tx2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaled_cluster = scaler.fit_transform(user_df)\n",
    "cluster_scaled = pd.DataFrame(scaled_cluster, columns=user_df.columns, index=user_df.index)\n",
    "\n",
    "\n",
    "data_corr = cluster_scaled.corr()\n",
    "sb.heatmap(data_corr, cmap = 'bwr') #heatmap of correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "clus_train, clus_test = sklearn.model_selection.train_test_split(cluster_scaled, test_size=0.3, random_state=123)\n",
    "\n",
    "clusters = range(1,10)\n",
    "meandist=[]\n",
    "\n",
    "for k in clusters:\n",
    "    model = sklearn.cluster.KMeans(n_clusters = k)\n",
    "    model.fit(clus_train)\n",
    "    clusassign=model.predict(clus_train)\n",
    "    meandist.append(sum(np.min(sp.spatial.distance.cdist(clus_train,model.cluster_centers_,'euclidean'),axis=1))/clus_train.shape[0])\n",
    "    \n",
    "plt.plot(clusters, meandist, '-o')\n",
    "#plt.subplot(2,1,1)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method')\n",
    "plt.show\n",
    "\n",
    "# # Convert DataFrame to matrix\n",
    "# mat = cluster_scaled.values\n",
    "# distorsions = []\n",
    "# x = range(2, 20)\n",
    "# for k in x:\n",
    "#     #Perform K Means\n",
    "#     kmeans = sk.cluster.KMeans(n_clusters=k)\n",
    "#     kmeans.fit(mat)\n",
    "#     distorsions.append(kmeans.inertia_)\n",
    "\n",
    "# fig = plt.figure(figsize=(15, 5))\n",
    "# plt.plot(x, distorsions)\n",
    "# plt.grid(True)\n",
    "# plt.title('Elbow curve')\n",
    "# plt.show\n",
    "# plt.xticks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().clear()\n",
    "model = sklearn.cluster.KMeans(n_clusters = 2)\n",
    "model.fit(clus_train)\n",
    "clusassign = model.predict(clus_train)\n",
    "\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "#Principal Component Analysis\n",
    "pca_2 = sklearn.decomposition.PCA(2)\n",
    "plot_columns = pca_2.fit_transform(clus_train)    \n",
    "plt.scatter(x=plot_columns[:,0],y=plot_columns[:,1],c=model.labels_,cmap = matplotlib.colors.ListedColormap(colors),edgecolors = 'none')\n",
    "plt.xlabel('Canonical variable 1')\n",
    "plt.ylabel('Canonical variable 2')\n",
    "plt.title('Scatterplot of Canonical Variables for 2 clusters')\n",
    "plt.show\n",
    "\n",
    "\n",
    "# Get cluster assignment labels\n",
    "labels = model.labels_\n",
    "# Format results as a DataFrame\n",
    "data = {'transaction_id':clus_train.index,'cluster_label':labels}\n",
    "results = pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
